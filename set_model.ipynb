{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import catboost as cat\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import random\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_FOLDER = '../data'\n",
    "\n",
    "\n",
    "kernel_train_path = 'memory_sample_kernel_log_round1_a_train.csv'\n",
    "failure_train_path = 'memory_sample_failure_tag_round1_a_train.csv'\n",
    "adress_train_path = 'memory_sample_address_log_round1_a_train.csv'\n",
    "\n",
    "\n",
    "kernel_test_path = 'memory_sample_kernel_log_round1_b_test.csv'\n",
    "failure_test_path = 'memory_sample_failure_tag_round1_b_test.csv'\n",
    "adress_test_path = 'memory_sample_address_log_round1_b_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(x):\n",
    "    if x<=60*60*24*7:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_var = ['1_hwerr_f', '1_hwerr_e', '2_hwerr_c', '2_sel', '3_hwerr_n', '2_hwerr_s', '3_hwerr_m', '1_hwerr_st',\n",
    "       '1_hw_mem_c', '3_hwerr_p', '2_hwerr_ce', '3_hwerr_as', '1_ke', '2_hwerr_p', '3_hwerr_kp', '1_hwerr_fl', '3_hwerr_r', '_hwerr_cd',\n",
    "       '3_sup_mce_note', '3_cmci_sub', '3_cmci_det', '3_hwerr_pi', '3_hwerr_o', '3_hwerr_mce_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_kernel(path, agg_time,time):\n",
    "    data = pd.read_csv(os.path.join(PARENT_FOLDER, path))\n",
    "    data['collect_time'] = pd.to_datetime(data['collect_time']).dt.floor(agg_time)\n",
    "    data['count'+time] = 1\n",
    "    \n",
    "    if 'tag' in data.columns:\n",
    "        del data['tag']\n",
    "        del data['failure_time']\n",
    "    \n",
    "    group_data = data.groupby(['serial_number','manufacturer','vendor','collect_time'],as_index=False).agg('sum')\n",
    "\n",
    "    return group_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(label_path, kernel_df):\n",
    "    failure_tag = pd.read_csv(os.path.join(PARENT_FOLDER,label_path))\n",
    "    failure_tag['failure_time']= pd.to_datetime(failure_tag['failure_time'])\n",
    "    merged_kernel = pd.merge(kernel_df,failure_tag[['serial_number',\n",
    "                                               'manufacturer','vendor','failure_time']],how='left',on=['serial_number',\n",
    "                                                                                                       'manufacturer','vendor'])\n",
    "    merged_kernel['diff_seconds'] = (merged_kernel['failure_time'] - merged_kernel['collect_time']).dt.days*24*60*60 \\\n",
    "                                + ((merged_kernel['failure_time']-merged_kernel['collect_time']).dt.seconds)\n",
    "    merged_kernel['label'] = merged_kernel['diff_seconds'].map(label)\n",
    "    return merged_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 评分函数，引用自PAKDD2021论坛中的score_func_round2，链接见readme\n",
    "def score_func_round2(sub_df, cur_failure_tag, verbose=False):\n",
    "    '''\n",
    "    注意cur_failure_tag的时间窗口要和sub_list一样。\n",
    "    sub_list需要加一个collect_time, 表示做出预测的那一分钟, 类型为 pd.Timestamp。\n",
    "    [{\"serial_number\":server_1, \"pti\":14, 'collect_time': Timestamp('2019-08-01 05:18:00')},\n",
    "    {\"serial_number\":server_123, \"pti\":1200, 'collect_time': Timestamp('2019-08-02 00:08:00')}]\n",
    "    '''\n",
    "    if sub_df.empty:\n",
    "        print(\"[Warning] sub num 0\")\n",
    "        return 0\n",
    "    # remove invalid sub\n",
    "    # 删除 ati < 0的提交\n",
    "    sub_df = sub_df.join(cur_failure_tag.set_index('serial_number')['failure_time'], how='left', on='serial_number')\n",
    "    sub_df['ati'] = (sub_df['failure_time']-sub_df['collect_time'])/pd.Timedelta('1min')\n",
    "    sub_df = sub_df[(sub_df['ati']>=0)|(sub_df['ati'].isna())]\n",
    "    # 取每个周期第一个\n",
    "    sub_df = sub_df.sort_values(by=['serial_number', 'collect_time'])\n",
    "    pre_ser = -1\n",
    "    init_pre_time = pd.to_datetime('2018-12-01')\n",
    "    window_time = pd.Timedelta('7D')\n",
    "    pre_time = init_pre_time\n",
    "    judge = []\n",
    "    for sn, cur_time in sub_df[['serial_number', 'collect_time']].values:\n",
    "        if pre_ser != sn:\n",
    "            pre_time = init_pre_time\n",
    "        if (cur_time-pre_time) < window_time:\n",
    "            judge.append(0)\n",
    "        else:\n",
    "            judge.append(1)\n",
    "            pre_time = cur_time\n",
    "        pre_ser = sn\n",
    "    judge = np.array(judge)\n",
    "    sub_df = sub_df[judge==1].reset_index(drop=True)\n",
    "\n",
    "    # failure_time_dict = cur_failure_tag.set_index('serial_number')['failure_time'].to_dict()\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    # score\n",
    "    n_pp = len(sub_df)\n",
    "    n_pr = len(cur_failure_tag)\n",
    "\n",
    "    n_tpr = 0\n",
    "    n_tpp = 0\n",
    "    for sn, pti, ati in sub_df[['serial_number', 'pti', 'ati']].values:\n",
    "        if pd.notna(ati):\n",
    "            if 0 <= pti < 7*24*60: # 待确定\n",
    "                if pti <= ati:\n",
    "                    n_tpp += sigmoid(pti/ati)\n",
    "            if ati < 7*24*60:\n",
    "                n_tpr += 1\n",
    "    \n",
    "    precision = n_tpp/n_pp\n",
    "    recall = n_tpr/n_pr\n",
    "    if (precision+recall) == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "    if verbose:\n",
    "        print(f'n_tpp: {n_tpp}, n_pp: {n_pp}, precision: {precision}, n_tpr: {n_tpr}, n_pr: {n_pr}, recall: {recall}, f1: {f1}')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_tag = pd.read_csv('../data/memory_sample_failure_tag_round1_a_train.csv')\n",
    "failure_tag['failure_time']= pd.to_datetime(failure_tag['failure_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_data = pd.read_csv('../data/memory_sample_failure_tag_round1_b_test.csv')\n",
    "failure_data['failure_time'] = pd.to_datetime(failure_data['failure_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取kernel表特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_train_2min = etl_kernel(kernel_train_path,'2min','2min')\n",
    "kernel_test_2min = etl_kernel(kernel_test_path,'2min','2min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取mce表特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mce_a = pd.read_csv(\"../data/memory_sample_mce_log_round1_a_train.csv\")\n",
    "mce_b = pd.read_csv(\"../data/memory_sample_mce_log_round1_b_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mce_data(df_mce,agg_time,time):\n",
    "    df = df_mce.copy()\n",
    "    for i in [\"Z\", \"AP\", \"G\", \"F\", \"BB\", \"E\", \"CC\", \"AF\", \"AE\"]:\n",
    "        df['mca_'+i+time] = (df.mca_id == i).astype(\"float\")\n",
    "    for i in [0, 1, 2, 3]:\n",
    "        df['trans_'+str(i)+time] = (df.transaction == i).astype(\"float\")\n",
    "    df['collect_time'] = pd.to_datetime(df['collect_time']).dt.floor(agg_time)\n",
    "\n",
    "    df['count_mce_'+time] = 1\n",
    "    mce_new = df.groupby(['serial_number','manufacturer','vendor','collect_time'],as_index=False).agg('sum')\n",
    "    return mce_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mce_train_2min = get_mce_data(mce_a,'2min','2min')\n",
    "mce_test_2min = get_mce_data(mce_b,'2min','2min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(kernel_train_2min, mce_train_2min,how='left',on=['serial_number','manufacturer','vendor','collect_time'])\n",
    "test = pd.merge(kernel_test_2min, mce_test_2min,how='left',on=['serial_number','manufacturer','vendor','collect_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_train_2min_sta = kernel_train_2min[['serial_number','count2min']].groupby(['serial_number'],as_index=False).agg(list)\n",
    "kernel_train_2min_sta['mean_2min'] = kernel_train_2min_sta['count2min'].apply(lambda x: np.mean(x))\n",
    "kernel_train_2min_sta['median_2min'] = kernel_train_2min_sta['count2min'].apply(lambda x: np.median(x))\n",
    "kernel_train_2min_sta['sum_2min'] = kernel_train_2min_sta['count2min'].apply(lambda x: sum(x))\n",
    "train = pd.merge(train,kernel_train_2min_sta[['serial_number','mean_2min','median_2min','sum_2min']],how='left',on=['serial_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_test_2min_sta = kernel_test_2min[['serial_number','count2min']].groupby(['serial_number'],as_index=False).agg(list)\n",
    "kernel_test_2min_sta['mean_2min'] = kernel_test_2min_sta['count2min'].apply(lambda x: np.mean(x))\n",
    "kernel_test_2min_sta['median_2min'] = kernel_test_2min_sta['count2min'].apply(lambda x: np.median(x))\n",
    "kernel_test_2min_sta['sum_2min'] = kernel_test_2min_sta['count2min'].apply(lambda x: sum(x))\n",
    "test = pd.merge(test,kernel_test_2min_sta[['serial_number','mean_2min','median_2min','sum_2min']],how='left',on=['serial_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取address特征文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_sta_train = pd.read_csv(\"address_sta_train.csv\")\n",
    "address_sta_test = pd.read_csv(\"address_sta_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_sta_train['collect_time'] = pd.to_datetime(address_sta_train['collect_time'])\n",
    "address_sta_test['collect_time'] = pd.to_datetime(address_sta_test['collect_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = pd.merge(train,address_sta_train[['serial_number', 'collect_time',\n",
    "       'row_num', 'row_max', 'row_repeat', 'col_num', 'col_max', 'col_repeat',\n",
    "       ]],how='left',on = ['serial_number','collect_time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = pd.merge(test,address_sta_test[['serial_number', 'collect_time',\n",
    "       'row_num', 'row_max', 'row_repeat', 'col_num', 'col_max', 'col_repeat',\n",
    "       ]],how='left',on = ['serial_number','collect_time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = getLabel(failure_train_path, train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['manufacturer', 'vendor', '1_hwerr_f','1_hwerr_e','2_hwerr_c', '3_hwerr_as','1_ke','3_hwerr_kp','3_sup_mce_note','2_hwerr_p', 'count2min',\n",
    "       'mca_Z2min', 'mca_AP2min', 'mca_G2min', 'mca_BB2min',\n",
    "       'mca_E2min', 'mca_CC2min', 'mca_AF2min',  'trans_02min',\n",
    "       'trans_12min', 'trans_22min', 'trans_32min', 'count_mce_2min',\n",
    "       'mean_2min', 'median_2min', 'sum_2min',\n",
    "       'row_num', 'row_max', 'row_repeat', 'col_num', 'col_max', 'col_repeat',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = pd.merge(test_new,failure_data[['serial_number',\n",
    "                                               'manufacturer','vendor','failure_time']],how='left',on=['serial_number','manufacturer','vendor'])\n",
    "test_new['diff_seconds'] = (test_new['failure_time'] - test_new['collect_time']).dt.days*24*60*60 + ((test_new['failure_time']-test_new['collect_time']).dt.seconds)\n",
    "test_new['label'] = test_new['diff_seconds'].map(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将1-5月数据与7月前20天数据合并\n",
    "test_7 = test_new[(test_new['collect_time']>=pd.to_datetime('20190701'))&(test_new['collect_time']<pd.to_datetime('20190722'))]\n",
    "train_concat = pd.concat([train_new,test_7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catboost建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cat = cat.CatBoostClassifier(learning_rate=0.01,iterations=1000,depth=6\n",
    "                                 ,colsample_bylevel = 0.8\n",
    "                                )\n",
    "\n",
    "clf_r_cat = cat.CatBoostRegressor(learning_rate=0.01,iterations=1000,depth=6,loss_function ='MAPE'\n",
    "                                  ,colsample_bylevel = 0.8\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_0 = train_concat[train_concat['label']==0].sample(len(train_concat[train_concat['label']==1])*10,random_state=2)\n",
    "sample = sample_0.append(train_concat[train_concat['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime('20190722')\n",
    "end = start + timedelta(days=10)\n",
    "test_ = test_new[(test_new['collect_time']>=start)&(test_new['collect_time']<end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['manufacturer','vendor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['vendor'] = sample['vendor'].apply(lambda x: int(x))\n",
    "test_['vendor'] = test_['vendor'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32394350408672246"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cat.fit(sample[feats],sample['label'],verbose=False,cat_features = cat_features)\n",
    "y_pred = clf_cat.predict_proba(test_[feats])[:,1]\n",
    "test_['cat'] = y_pred\n",
    "test_p = test_[test_['cat']>=0.8]\n",
    "fail_7 = sample[sample['label']==1]\n",
    "fail_7['diff_min'] = np.floor(fail_7['diff_seconds']/60)+2\n",
    "clf_r_cat.fit(fail_7[feats],fail_7['diff_min'],verbose=False)\n",
    "y_pred = clf_r_cat.predict(test_p[feats])\n",
    "test_p['pti'] = np.ceil(y_pred)\n",
    "res = test_p[['serial_number','collect_time','pti']]\n",
    "fail = failure_data[(failure_data['failure_time']>=start)&(failure_data['failure_time']<end)]\n",
    "fail['failure_time'] = pd.to_datetime(fail['failure_time'])\n",
    "score_func_round2(res, fail, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对回归结果做一些规则性处理，提高测评分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.353634142418905"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p['reg'] = np.ceil(y_pred)\n",
    "def deal_pti(x):\n",
    "    if x>40:\n",
    "        return 4000\n",
    "    elif x>30:\n",
    "        return 200\n",
    "    elif x>20:\n",
    "        return 30\n",
    "    else:\n",
    "        return 10\n",
    "        \n",
    "test_p['pti'] = test_p['reg'].apply(lambda x: deal_pti(x))       \n",
    "res = test_p[['serial_number','collect_time','pti']]\n",
    "score_func_round2(res, fail, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('p1.pickle','wb')as f:\n",
    "    pickle.dump(clf_cat,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('p2.pickle','wb')as f:\n",
    "    pickle.dump(clf_r_cat,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
